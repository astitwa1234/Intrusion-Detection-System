import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split


data=pd.read_csv('./Train_data.csv')
data.head()


data.info()


data.describe()

data.shape

#checking for missing values
length=data.shape[0]
miss_col=[i for i in data.columns if data[i].isnull().sum()>0]
print ("No of columns with missing values {miss_col}")

duplicates=data.duplicated().sum()
print("no of duplicated rows:{duplicates}")


#performing label encoding 
from sklearn.preprocessing import LabelEncoder
def label(df):
    for i in df.columns:
        if df[i].dtype=='object':
            label_encoder=LabelEncoder()
            df[i]=label_encoder.fit_transform(df[i])
label(data)


#checking for outlier
for col in data:
    q1=data[col].quantile(0.25)
    q3=data[col].quantile(0.75)
    iqf=q3-q1
    lower=q1-iqf*1.5
    upper=q3+iqf*1.5
    outlier=data[(data[col]>upper)& data[col]<lower]
print(f"outlier information:\n{outlier.sum()}")
    

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
import itertools
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

X_train=data.drop('xAttack', axis=1)
Y_train=data['xAttack']

rfc=RandomForestClassifier()
rfe=RFE(rfc, n_features_to_select=10)
rfe=rfe.fit(X_train, Y_train)

feature_map=[(i,v) for i,v in itertools.zip_longest(rfe.get_support(), X_train.columns)]
selected_features=[v for i,v in feature_map if i==True]
selected_features

X_train=X_train[selected_features]
X_train

from sklearn.preprocessing import StandardScaler
scale=StandardScaler()
X_train=scale.fit_transform(X_train)


x_train,x_test,y_train,y_test=train_test_split(X_train,Y_train,train_size=0.70,random_state=2)
x_train.shape
x_test.shape
y_train.shape
y_test.shape


#naive Bayes classifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve,f1_score,recall_score
nb_model=GaussianNB()
nb_model=nb_model.fit(x_train,y_train)

prediction=nb_model.predict(x_test)
accuracy_nb=accuracy_score(y_test,prediction)
accuracy_nb

f1=f1_score(y_test,prediction,average='weighted')
print(f'F1 Score: {f1}')



#Decision Tree

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve,f1_score,recall_score
dt_model=DecisionTreeClassifier()
dt_model=dt_model.fit(x_train,y_train)

dt_prediction=dt_model.predict(x_test)
dt_accuracy=accuracy_score(y_test,dt_prediction)
dt_accuracy
f1_dt=f1_score(y_test,dt_prediction,average='weighted')
print(f'F1 Score: {f1_dt}')



#KNN model
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve,f1_score
kn_model=KNeighborsClassifier()
kn_model=kn_model.fit(x_train,y_train)

k_prediction=kn_model.predict(x_test)
k_accuracy=accuracy_score(y_test,k_prediction)
k_accuracy

f1_k=f1_score(y_test,k_prediction,average='weighted')
print(f'F1 Score: {f1_k}')


#logistic Regression

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve,f1_score

lr_classifier=LogisticRegression()
lr_classifier.fit(x_train, y_train)

y_pred_lr=lr_classifier.predict(x_test)
lr_accuracy=accuracy_score(y_test, y_pred_lr)
lr_accuracy

f1_lr=f1_score(y_pred_lr,k_prediction,average='weighted')
print(f'F1 Score: {f1_lr}')

# Print the results
print("Logistic Regression:")
print(f"  Accuracy: {lr_accuracy:.5f}")
print(f"  F1 Score: {f1_lr:.5f}")

print("\nK-Nearest Neighbors (k=3):")
print(f"  Accuracy: {k_accuracy:.5f}")
print(f"  F1 Score: {f1_k:.5f}")

print("\nNaive Bayes (k=3):")
print(f"  Accuracy: {accuracy_nb:.5f}")
print(f"  F1 Score: {f1:.5f}")

print("\nDecision Tree (k=3):")
print(f"  Accuracy: {dt_accuracy:.5f}")
print(f"  F1 Score: {f1_dt:.5f}")
